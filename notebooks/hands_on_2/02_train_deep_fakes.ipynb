{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Reenactment DeepFake using Pix2Pix GAN\n",
    "\n",
    "- Pix2Pix is a paired image to image translation architecture\n",
    "- In this notebook, we will leverage a pix2pix architecture to train for reenactment\n",
    "- The GAN will be trained with input as facial landmarks and output as actual video frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Libraries & Utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gan_utils import downsample_block, upsample_block, discriminator_block\n",
    "from data_utils import plot_sample_images, batch_generator, get_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (8,8),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "\n",
    "plt.rcParams.update(params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Generator\n",
    "> U-Net Generator\n",
    "\n",
    "The U-Net architecture uses skip connections to shuttle important features between the input and outputs. In case of pix2pix GAN, skip connections are added between every ith down-sampling and (n-i)th over-sampling layers, where n is the total number of layers in the generator. The skip connection leads to concatenation of all channels from the ith and (n-i)th layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_generator(img_shape,channels=3,num_filters=64):\n",
    "    # Image input\n",
    "    input_layer = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    down_sample_1 = downsample_block(input_layer, \n",
    "                                     num_filters, \n",
    "                                     batch_normalization=False)\n",
    "    # rest of the down-sampling blocks have batch_normalization=true\n",
    "    down_sample_2 = downsample_block(down_sample_1, num_filters*2)\n",
    "    down_sample_3 = downsample_block(down_sample_2, num_filters*4)\n",
    "    down_sample_4 = downsample_block(down_sample_3, num_filters*8)\n",
    "    down_sample_5 = downsample_block(down_sample_4, num_filters*8)\n",
    "    down_sample_6 = downsample_block(down_sample_5, num_filters*8)\n",
    "    down_sample_7 = downsample_block(down_sample_6, num_filters*8)\n",
    "\n",
    "    # Upsampling blocks with skip connections\n",
    "    upsample_1 = upsample_block(down_sample_7, down_sample_6, num_filters*8)\n",
    "    upsample_2 = upsample_block(upsample_1, down_sample_5, num_filters*8)\n",
    "    upsample_3 = upsample_block(upsample_2, down_sample_4, num_filters*8)\n",
    "    upsample_4 = upsample_block(upsample_3, down_sample_3, num_filters*8)\n",
    "    upsample_5 = upsample_block(upsample_4, down_sample_2, num_filters*2)\n",
    "    upsample_6 = upsample_block(upsample_5, down_sample_1, num_filters)\n",
    "\n",
    "    upsample_7 = UpSampling2D(size=2)(upsample_6)\n",
    "    output_img = Conv2D(channels, \n",
    "                        kernel_size=4, \n",
    "                        strides=1, \n",
    "                        padding='same', \n",
    "                        activation='tanh')(upsample_7)\n",
    "\n",
    "    return Model(input_layer, output_img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Patch-GAN Discriminator\n",
    "The authors for pix2pix propose a Patch-GAN setup for the discriminator which takes the required inputs and generates an output of size NxN. Each $x_{ij}$ element of the NxN output signifies whether the corresponding patch ij in the generated image is real or fake. Each output patch can be traced back to its initial input patch basis the effective receptive field for each of the layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_discriminator(img_shape,num_filters=64):\n",
    "    input_img = Input(shape=img_shape)\n",
    "    cond_img = Input(shape=img_shape)\n",
    "\n",
    "    # Concatenate input and conditioning image by channels \n",
    "    # as input for discriminator\n",
    "    combined_input = Concatenate(axis=-1)([input_img, cond_img])\n",
    "\n",
    "    # First discriminator block does not use batch_normalization\n",
    "    disc_block_1 = discriminator_block(combined_input, \n",
    "                                       num_filters, \n",
    "                                       batch_normalization=False)\n",
    "    disc_block_2 = discriminator_block(disc_block_1, num_filters*2)\n",
    "    disc_block_3 = discriminator_block(disc_block_2, num_filters*4)\n",
    "    disc_block_4 = discriminator_block(disc_block_3, num_filters*8)\n",
    "\n",
    "    output = Conv2D(1, kernel_size=4, strides=1, padding='same')(disc_block_4)\n",
    "\n",
    "    return Model([input_img, cond_img], output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Training Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(generator, \n",
    "          discriminator, \n",
    "          gan, \n",
    "          patch_gan_shape, \n",
    "          epochs,\n",
    "          path='/content/data' ,\n",
    "          batch_size=1, \n",
    "          sample_interval=50):\n",
    "  \n",
    "    # Ground truth shape/ Patch-GAN outputs\n",
    "    real_y = np.ones((batch_size,) + patch_gan_shape)\n",
    "    fake_y = np.zeros((batch_size,) + patch_gan_shape)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch={}\".format(epoch))\n",
    "        for idx, (imgs_source, imgs_cond) in enumerate(batch_generator(path=path,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     img_res=[IMG_HEIGHT, IMG_WIDTH])):\n",
    "            # train disriminator\n",
    "            # generator generates outputs based on conditioned input images\n",
    "            fake_imgs = generator.predict([imgs_cond])\n",
    "\n",
    "            # calculate discriminator loss on real samples\n",
    "            disc_loss_real = discriminator.train_on_batch([imgs_source, imgs_cond], \n",
    "                                                       real_y)\n",
    "            # calculate discriminator loss on fake samples\n",
    "            disc_loss_fake = discriminator.train_on_batch([fake_imgs, imgs_cond], \n",
    "                                                       fake_y)\n",
    "            # overall discriminator loss\n",
    "            discriminator_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake)\n",
    "\n",
    "            # train generator\n",
    "            gen_loss = gan.train_on_batch([imgs_source, imgs_cond],\n",
    "                                          [real_y, imgs_source])\n",
    "\n",
    "            # training updates every 50 iterations\n",
    "            if idx % 50 == 0:\n",
    "                print (\"[Epoch {}/{}] [Discriminator loss: {}, accuracy: {}] [Generato r loss: {}]\".format(epoch, \n",
    "                                                                                                        epochs,\n",
    "                                                                                                        discriminator_loss[0], \n",
    "                                                                                                        100*discriminator_loss[1],\n",
    "                                                                                                        gen_loss[0]))\n",
    "\n",
    "                # Plot and Save progress every few iterations\n",
    "                if idx % sample_interval == 0:\n",
    "                    plot_sample_images(generator=generator,\n",
    "                                     path=path,\n",
    "                                     epoch=epoch,\n",
    "                                     batch_num=idx,\n",
    "                                     output_dir='images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "discriminator = build_discriminator(img_shape=(IMG_HEIGHT,IMG_WIDTH,3),\n",
    "                                    num_filters=64)\n",
    "discriminator.compile(loss='mse',\n",
    "                      optimizer=Adam(0.0002, 0.5),\n",
    "                      metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generator = build_generator(img_shape=(IMG_HEIGHT,IMG_WIDTH,3),\n",
    "                            channels=3,\n",
    "                            num_filters=64)\n",
    "\n",
    "source_img = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "cond_img = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "fake_img = generator(cond_img)\n",
    "\n",
    "discriminator.trainable = False\n",
    "output = discriminator([fake_img, cond_img])\n",
    "\n",
    "gan = Model(inputs=[source_img, cond_img], outputs=[output, fake_img])\n",
    "gan.compile(loss=['mse', 'mae'],\n",
    "            loss_weights=[1, 100],\n",
    "            optimizer=Adam(0.0002, 0.5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prepare patch size for our setup\n",
    "patch = int(IMG_HEIGHT / 2**4)\n",
    "patch_gan_shape = (patch, patch, 1)\n",
    "print(\"Patch Shape={}\".format(patch_gan_shape))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.keras.utils.plot_model(generator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Begins!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(generator, discriminator, gan, patch_gan_shape, epochs=200, batch_size=1, sample_interval=200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "generator.save('gen_pix2pix_deepfake_obama.h5',save_format='h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "discriminator.save('dis_pix2pix_deepfake_obama.h5',save_format='h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "gan.save('gan_pix2pix_deepfake_obama.h5',save_format='h5')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}